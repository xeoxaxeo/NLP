{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. 데이터 준비 및 인덱싱 - 샘플링O 후처리O\n",
    "- 데이터 후처리: 자모/특수문자 반복 제거, 불용어 제거\n",
    "- 샘플링: 앞에서부터 5000개"
   ],
   "id": "63918b857d431907"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T15:53:52.069408Z",
     "start_time": "2025-11-26T15:34:49.596113Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 데이터 로드 중\n",
      "샘플링: 전체 50,222개 중 앞쪽 5,000개 사용\n",
      "Index 구축 시작 (Target: inverted_index_sample_clean.db)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Indexing:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d53bb9e00f5c4cc4a20e8417ed6f48ad"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Saving Inverted Index:   0%|          | 0/153179 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f3e51e3fd7d4d8d86a4698131350c9e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 구축 완료\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "from kiwipiepy import Kiwi\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "DB_DIR = PROJECT_ROOT / 'database'\n",
    "\n",
    "DB_PATH = DB_DIR / 'inverted_index_sample_clean.db'\n",
    "\n",
    "for d in [DATA_DIR, DB_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# 데이터 로드 및 5000개 샘플링\n",
    "if not (DATA_DIR / 'corpus.pkl').exists():\n",
    "    print(\"데이터셋 다운로드 중\")\n",
    "    queries = load_dataset(\"junyoungson/KomuRetrieval\", \"queries\", split=\"queries\")\n",
    "    corpus = load_dataset(\"junyoungson/KomuRetrieval\", \"corpus\", split=\"corpus\")\n",
    "    qrels = load_dataset(\"junyoungson/KomuRetrieval\", split=\"test\")\n",
    "\n",
    "    full_corpus = [item for item in tqdm(corpus, desc=\"Corpus Loading\")]\n",
    "    full_queries = [item for item in tqdm(queries, desc=\"Queries Loading\")]\n",
    "    full_qrels = [item for item in tqdm(qrels, desc=\"Qrels Loading\")]\n",
    "\n",
    "    with open(DATA_DIR / 'corpus.pkl', 'wb') as f:\n",
    "        pickle.dump(full_corpus, f)\n",
    "    with open(DATA_DIR / 'queries.pkl', 'wb') as f:\n",
    "        pickle.dump(full_queries, f)\n",
    "    with open(DATA_DIR / 'qrels.pkl', 'wb') as f:\n",
    "        pickle.dump(full_qrels, f)\n",
    "else:\n",
    "    print(\"기존 데이터 로드 중\")\n",
    "    with open(DATA_DIR / 'corpus.pkl', 'rb') as f:\n",
    "        full_corpus = pickle.load(f)\n",
    "\n",
    "SAMPLE_SIZE = 5000\n",
    "corpus = full_corpus[:SAMPLE_SIZE]\n",
    "print(f\"샘플링: 전체 {len(full_corpus):,}개 중 앞쪽 {len(corpus):,}개 사용\")\n",
    "\n",
    "kiwi = Kiwi(num_workers=-1)\n",
    "\n",
    "# 불용어 리스트\n",
    "STOPWORDS = {\n",
    "    '때문', '아니', '나오', '보이', '이후', '정도', '위하', '가지', '경우',\n",
    "    '사실', '만들', '사람', '시작', '가능', '대하', '자신', '등장', '모습',\n",
    "    '문제', '부분', '이것', '그것'\n",
    "}\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    if not text: return []\n",
    "\n",
    "    # Null 문자 제거\n",
    "    text = text.replace('\\x00', '')\n",
    "\n",
    "    # 자모, 특수문자 반복 제거\n",
    "    text = re.sub(r'[ㄱ-ㅎㅏ-ㅣ]+', '', text)\n",
    "    text = re.sub(r'[\\.\\?\\!~\\-]{2,}', '.', text)\n",
    "\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        useful_tags = ['NNG', 'NNP', 'VV', 'VA', 'MAG']\n",
    "\n",
    "        result = []\n",
    "        for t in tokens:\n",
    "            if t.tag in useful_tags and len(t.form) > 1:\n",
    "                # 불용어 필터링\n",
    "                if t.form not in STOPWORDS:\n",
    "                    result.append(t.form)\n",
    "        return result\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# DB 구축\n",
    "def build_index_db(corpus):\n",
    "    conn = sqlite3.connect(str(DB_PATH))\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for table in ['documents', 'inverted_index', 'statistics', 'term_stats']:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table}\")\n",
    "\n",
    "    cursor.execute(\"CREATE TABLE documents (doc_id TEXT PRIMARY KEY, title TEXT, length INTEGER, tokens TEXT)\")\n",
    "    cursor.execute(\"CREATE TABLE inverted_index (term TEXT, doc_id TEXT, tf INTEGER, PRIMARY KEY (term, doc_id))\")\n",
    "    cursor.execute(\"CREATE TABLE statistics (key TEXT PRIMARY KEY, value REAL)\")\n",
    "    cursor.execute(\"CREATE TABLE term_stats (term TEXT PRIMARY KEY, df INTEGER)\")\n",
    "\n",
    "    inverted_index = defaultdict(lambda: defaultdict(int))\n",
    "    doc_lengths = {}\n",
    "    doc_data = []\n",
    "\n",
    "    print(f\"Index 구축 시작 (Target: {DB_PATH.name})\")\n",
    "    for doc in tqdm(corpus, desc=\"Indexing\"):\n",
    "        doc_id = doc['_id']\n",
    "        full_text = f\"{doc['title']} {doc['text']}\"\n",
    "        tokens = tokenize(full_text)\n",
    "\n",
    "        doc_lengths[doc_id] = len(tokens)\n",
    "        doc_data.append((doc_id, doc['title'], len(tokens), json.dumps(tokens, ensure_ascii=False)))\n",
    "\n",
    "        counts = Counter(tokens)\n",
    "        for term, tf in counts.items():\n",
    "            inverted_index[term][doc_id] = tf\n",
    "\n",
    "        if len(doc_data) >= 1000:\n",
    "            cursor.executemany(\"INSERT INTO documents VALUES (?, ?, ?, ?)\", doc_data)\n",
    "            conn.commit()\n",
    "            doc_data = []\n",
    "\n",
    "    if doc_data:\n",
    "        cursor.executemany(\"INSERT INTO documents VALUES (?, ?, ?, ?)\", doc_data)\n",
    "        conn.commit()\n",
    "\n",
    "    index_data = []\n",
    "    term_stats_data = []\n",
    "\n",
    "    for term, postings in tqdm(inverted_index.items(), desc=\"Saving Inverted Index\"):\n",
    "        term_stats_data.append((term, len(postings)))\n",
    "        for doc_id, tf in postings.items():\n",
    "            index_data.append((term, doc_id, tf))\n",
    "\n",
    "        if len(index_data) >= 10000:\n",
    "            cursor.executemany(\"INSERT INTO inverted_index VALUES (?, ?, ?)\", index_data)\n",
    "            conn.commit()\n",
    "            index_data = []\n",
    "\n",
    "    if index_data:\n",
    "        cursor.executemany(\"INSERT INTO inverted_index VALUES (?, ?, ?)\", index_data)\n",
    "\n",
    "    cursor.executemany(\"INSERT INTO term_stats VALUES (?, ?)\", term_stats_data)\n",
    "\n",
    "    # 통계 정보 저장\n",
    "    N = len(corpus)\n",
    "    avgdl = sum(doc_lengths.values()) / N if N > 0 else 0\n",
    "    cursor.executemany(\"INSERT INTO statistics VALUES (?, ?)\", [\n",
    "        ('N', N), ('avgdl', avgdl), ('total_terms', len(inverted_index))\n",
    "    ])\n",
    "\n",
    "    # 인덱스 생성\n",
    "    cursor.execute(\"CREATE INDEX idx_term ON inverted_index(term)\")\n",
    "    cursor.execute(\"CREATE INDEX idx_doc_id ON inverted_index(doc_id)\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"DB 구축 완료\")\n",
    "\n",
    "# 실행\n",
    "build_index_db(corpus)"
   ],
   "id": "e0f9d4c9faf49ef9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
