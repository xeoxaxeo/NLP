{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. 데이터 후처리",
   "id": "ded7adc83ac7b6d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:42:44.621115Z",
     "start_time": "2025-11-26T13:42:41.575418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "print(\"데이터 로드 중\")\n",
    "with open(DATA_DIR / 'corpus.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "print(f\"총 문서 수: {len(corpus):,}개\")"
   ],
   "id": "77769253c61f92b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중\n",
      "총 문서 수: 50,222개\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0-1. 한글 자모 (ㅋㅋ, ㅎㅎ, ㅠㅠ, ㅏㅏ 등) 찾기",
   "id": "e89681136397d34b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:49:38.165579Z",
     "start_time": "2025-11-26T13:49:37.534909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "jamo_pattern = re.compile(r'[ㄱ-ㅎㅏ-ㅣ]+')\n",
    "count = 0\n",
    "\n",
    "for doc in corpus[:5000]: # 앞 5000개만 확인\n",
    "    text = doc['text']\n",
    "    matches = jamo_pattern.findall(text)\n",
    "\n",
    "    if matches:\n",
    "        count += 1\n",
    "        if count <= 10: # 샘플 10개만 출력\n",
    "            print(f\"\\n[Doc ID: {doc['_id']}] {doc['title']}\")\n",
    "            for match in set(matches):\n",
    "                if len(match) > 1:\n",
    "                    idx = text.find(match)\n",
    "                    start = max(0, idx - 20)\n",
    "                    end = min(len(text), idx + 20)\n",
    "                    print(f\"발견: '{match}' -> ...{text[start:end]}...\")\n",
    "\n",
    "print(f\"\\n샘플 5000개 중 자모 포함 문서 수: {count}개 (약 {count/5000*100:.1f}%)\")"
   ],
   "id": "25672fdd225cb055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Doc ID: 휘핑크림] 휘핑크림\n",
      "\n",
      "[Doc ID: taboo tears you up] taboo tears you up\n",
      "\n",
      "[Doc ID: 오현민] 오현민\n",
      "발견: 'ㅜㅜ' -> ... 이현우가 아닌 키이스트의 이현우였어ㅜㅜ`라는 희대의 드립을 치기도......\n",
      "\n",
      "[Doc ID: 최건우] 최건우\n",
      "발견: 'ㅋㅋㅋㅋㅋㅋ' -> ...본에선 아니키(형님) 분위기 좀 읽어ㅋㅋㅋㅋㅋㅋ 같은 반응이 많이 나온 ...\n",
      "\n",
      "[Doc ID: 마쿠노우치 잇포] 마쿠노우치 잇포\n",
      "\n",
      "[Doc ID: 로아인(그랑블루 판타지 Versus)] 로아인(그랑블루 판타지 Versus)\n",
      "발견: 'ㅎㅎ' -> ... 있다. 승부처 ※G·M·J 해제됨다ㅎㅎ : 초일기당천 도중 ↓→→ + ...\n",
      "\n",
      "[Doc ID: 집파리] 집파리\n",
      "\n",
      "[Doc ID: 진주동명고등학교] 진주동명고등학교\n",
      "\n",
      "[Doc ID: 소닉 더 헤지혹] 소닉 더 헤지혹\n",
      "발견: 'ㅋㅋ' -> ...를 넘어섰다. \"움직일 수 없다니? ㅋㅋ 움직이고 있잖아! 애초에 지구는...\n",
      "\n",
      "[Doc ID: 정경두(프로게이머)] 정경두(프로게이머)\n",
      "\n",
      "샘플 5000개 중 자모 포함 문서 수: 466개 (약 9.3%)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0-2. 특수문자 과도한 반복 (!!!!, ...., ~~~) 찾기",
   "id": "302b408edea0dc3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:50:02.421849Z",
     "start_time": "2025-11-26T13:50:01.819235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "punct_pattern = re.compile(r'[\\.\\!\\?~\\-]{3,}')\n",
    "count = 0\n",
    "\n",
    "for doc in corpus[:5000]:\n",
    "    text = doc['text']\n",
    "    matches = punct_pattern.findall(text)\n",
    "\n",
    "    if matches:\n",
    "        count += 1\n",
    "        if count <= 10:\n",
    "            print(f\"\\n[Doc ID: {doc['_id']}] {doc['title']}\")\n",
    "            for match in set(matches):\n",
    "                if len(match) > 0:\n",
    "                    idx = text.find(match)\n",
    "                    start = max(0, idx - 20)\n",
    "                    end = min(len(text), idx + 20)\n",
    "                    print(f\"발견: '{match}' -> ...{text[start:end]}...\")\n",
    "\n",
    "print(f\"\\n샘플 5000개 중 반복 부호 포함 문서 수: {count}개 (약 {count/5000*100:.1f}%)\")"
   ],
   "id": "1db04757db4225de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Doc ID: 방관하는 초월자] 방관하는 초월자\n",
      "발견: '...' -> ...저게 사실인 건지 아니면 핑계인 건지... 수퍼내추럴 - 신 스타크래프트...\n",
      "\n",
      "[Doc ID: 제이드(스타크래프트)] 제이드(스타크래프트)\n",
      "발견: '...' -> ...터진 메카닉 병력은 별로 안 좋아하니... 그래서 이 맵을 기피하는 테란...\n",
      "\n",
      "[Doc ID: 개척시대(스타크래프트)] 개척시대(스타크래프트)\n",
      "발견: '...' -> ...기를 참고할 것. 당연히 스타 명경기...까지는 아니고 기네스에 등재되었...\n",
      "\n",
      "[Doc ID: 테란맵] 테란맵\n",
      "발견: '...' -> ... 핵 사일로는 어쨌든 고스트를 뽑아야... 모든 종족이 멀티를 먹기 힘든...\n",
      "\n",
      "[Doc ID: 라그나로크(스타크래프트)] 라그나로크(스타크래프트)\n",
      "발견: '...' -> ...호는 어쩌면 질 운명이었는지도 모른다... 사실 그 운명을 극복할 수도 ...\n",
      "\n",
      "[Doc ID: 스파클(스타크래프트)] 스파클(스타크래프트)\n",
      "발견: '...' -> ...쳐질 여지는 적다는 전망이 우세했으나... 3월 4일 최종 버전 패치 이...\n",
      "\n",
      "[Doc ID: REDALiCE] REDALiCE\n",
      "발견: '...' -> ... 시 작곡가 본인이 \"DJ NAGAI... 대체 누구냐...\" 라는 트윗...\n",
      "\n",
      "[Doc ID: CENSORED!!] CENSORED!!\n",
      "발견: '...' -> ...틀을 자유로이 사용해도 좋다며 뿌렸다... 2020년에 토파조 유튜브 구...\n",
      "\n",
      "[Doc ID: 오현민] 오현민\n",
      "발견: '...' -> ...있었다고 했으면서 유튜브에선 4~5명...? 1월, 페이스북에 오랜만에 ...\n",
      "발견: '...?' -> ...있었다고 했으면서 유튜브에선 4~5명...? 1월, 페이스북에 오랜만에 ...\n",
      "\n",
      "[Doc ID: 포테토칩] 포테토칩\n",
      "발견: '...' -> ... 돌아왔다. 건강스낵 더위까지 날려요...고구마·콩등 재료에 저칼로리제품...\n",
      "\n",
      "샘플 5000개 중 반복 부호 포함 문서 수: 2817개 (약 56.3%)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0-3. 취소선 내용 확인",
   "id": "45cd6b3534346c36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T13:59:04.234208Z",
     "start_time": "2025-11-26T13:59:04.206959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strike_pattern = re.compile(r'~~(.*?)~~')\n",
    "count = 0\n",
    "\n",
    "for doc in corpus[:5000]:\n",
    "    text = doc['text']\n",
    "    matches = list(strike_pattern.finditer(text))\n",
    "\n",
    "    if matches:\n",
    "        count += 1\n",
    "        if count <= 10:\n",
    "            print(f\"\\n[Doc ID: {doc['_id']}] {doc['title']}\")\n",
    "\n",
    "            for match in matches[:3]:\n",
    "                content = match.group(1)\n",
    "\n",
    "                if len(content.strip()) > 0:\n",
    "                    start_idx = match.start()\n",
    "                    end_idx = match.end()\n",
    "\n",
    "                    context_start = max(0, start_idx - 30)\n",
    "                    context_end = min(len(text), end_idx + 30)\n",
    "\n",
    "                    context_text = text[context_start:context_end]\n",
    "                    context_text = context_text.replace('\\n', ' ')\n",
    "\n",
    "                    print(f\"발견: '{match}' -> ...{text[start:end]}...\")\n",
    "\n",
    "print(f\"\\n샘플 5000개 중 취소선 포함 문서 수: {count}개 (약 {count/5000*100:.1f}%)\")"
   ],
   "id": "1b92f57fec3d909d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "샘플 5000개 중 취소선 포함 문서 수: 0개 (약 0.0%)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0-4. 불용어 후보 탐색",
   "id": "761e8f7cf380b330"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:20:23.373368Z",
     "start_time": "2025-11-26T15:17:11.100874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "kiwi = Kiwi(num_workers=-1)\n",
    "token_counter = Counter()\n",
    "\n",
    "# 1000개만 샘플링해서 토큰화\n",
    "for doc in tqdm(corpus[:1000], desc=\"Tokenizing Sample\"):\n",
    "    tokens = kiwi.tokenize(doc['text'])\n",
    "    useful_tags = ['NNG', 'NNP', 'VV', 'VA', 'MAG']\n",
    "\n",
    "    # 형태소만 추출해서 카운트\n",
    "    for t in tokens:\n",
    "        if t.tag in useful_tags and len(t.form) > 1:\n",
    "            token_counter[t.form] += 1\n",
    "\n",
    "print(\"\\n가장 많이 등장한 단어 Top 20\")\n",
    "for i, (word, count) in enumerate(token_counter.most_common(20), 1):\n",
    "    print(f\"{i}위: {word} ({count}회)\")"
   ],
   "id": "93d590a7f3ac2c1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizing Sample:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "149e8932678e4304834c229a9ded6729"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "가장 많이 등장한 단어 Top 20\n",
      "1위: 나오 (4330회)\n",
      "2위: 보이 (3781회)\n",
      "3위: 경기 (3308회)\n",
      "4위: 자신 (3258회)\n",
      "5위: 경우 (3210회)\n",
      "6위: 이후 (3209회)\n",
      "7위: 공격 (3066회)\n",
      "8위: 사용 (2982회)\n",
      "9위: 정도 (2953회)\n",
      "10위: 사람 (2943회)\n",
      "11위: 만들 (2511회)\n",
      "12위: 등장 (2493회)\n",
      "13위: 위하 (2428회)\n",
      "14위: 사실 (2325회)\n",
      "15위: 대하 (2280회)\n",
      "16위: 가능 (2253회)\n",
      "17위: 게임 (2004회)\n",
      "18위: 상대 (1931회)\n",
      "19위: 모습 (1896회)\n",
      "20위: 시작 (1852회)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T15:17:11.034878Z",
     "start_time": "2025-11-26T14:54:22.436588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "\n",
    "# 5000개 샘플 데이터 로드\n",
    "with open(DATA_DIR / 'corpus.pkl', 'rb') as f:\n",
    "    full_corpus = pickle.load(f)\n",
    "    corpus = full_corpus[:5000]\n",
    "\n",
    "print(f\"검증 대상 문서 수(N): {len(corpus):,}개\")\n",
    "\n",
    "# 불용어 후보 리스트\n",
    "candidate_stopwords = [\n",
    "    # 1. 진단 결과 상위 랭크\n",
    "    '나오', '보이', '자신', '경우', '이후', '정도', '사람',\n",
    "    '만들', '등장', '위하', '사실', '대하', '가능', '모습', '시작',\n",
    "    # 2. 일반적인 불용어\n",
    "    '있다', '없다', '하다', '되다', '않다', '그렇다', '아니',\n",
    "    '때문', '이것', '저것', '그것', '부분', '가지', '문제'\n",
    "]\n",
    "\n",
    "# 비교용 키워드\n",
    "candidate_stopwords.extend(['대학교', '인공지능', '자연'])\n",
    "\n",
    "# 빈도(DF) 카운팅\n",
    "kiwi = Kiwi(num_workers=-1)\n",
    "doc_freq = {word: 0 for word in candidate_stopwords}\n",
    "\n",
    "print(\"문서 스캔 중\")\n",
    "for doc in tqdm(corpus, desc=\"Scanning\"):\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(doc['text'])\n",
    "        # 한 문서 내 중복 제거\n",
    "        unique_tokens = set(t.form for t in tokens)\n",
    "\n",
    "        for word in candidate_stopwords:\n",
    "            if word in unique_tokens:\n",
    "                doc_freq[word] += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# IDF 계산 및 결과 출력\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'순위':<4} | {'단어':<10} | {'DF (등장 문서 수)':<15} | {'IDF 점수':<10} | {'판정'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "N = len(corpus)\n",
    "\n",
    "for word, df in doc_freq.items():\n",
    "    if df == 0:\n",
    "        idf = 0.0\n",
    "    else:\n",
    "        # IDF = log(N / DF)\n",
    "        idf = math.log(N / df)\n",
    "    results.append((word, df, idf))\n",
    "\n",
    "# IDF 점수 낮은 순(쓸모없는 순)으로 정렬\n",
    "results.sort(key=lambda x: x[2])\n",
    "\n",
    "for i, (word, df, idf) in enumerate(results, 1):\n",
    "    if word in ['반도체', '인공지능', '손흥민']:\n",
    "        judgement = \"키워드\"\n",
    "    elif idf < 1.5:  # 기준점: 1.5점 미만이면 불용어로 간주\n",
    "        judgement = \"삭제\"\n",
    "    else:\n",
    "        judgement = \"애매\"\n",
    "\n",
    "    print(f\"{i:<4} | {word:<10} | {df:<15} | {idf:.4f}     | {judgement}\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "id": "54a4934cf985e49a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 대상 문서 수(N): 5,000개\n",
      "문서 스캔 중\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Scanning:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "672ab82f02a44df48d4b1af89ba0065f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "순위   | 단어         | DF (등장 문서 수)    | IDF 점수     | 판정\n",
      "======================================================================\n",
      "1    | 있다         | 0               | 0.0000     | 삭제\n",
      "2    | 없다         | 0               | 0.0000     | 삭제\n",
      "3    | 그렇다        | 0               | 0.0000     | 삭제\n",
      "4    | 때문         | 3914            | 0.2449     | 삭제\n",
      "5    | 아니         | 3884            | 0.2526     | 삭제\n",
      "6    | 나오         | 3799            | 0.2747     | 삭제\n",
      "7    | 보이         | 3594            | 0.3302     | 삭제\n",
      "8    | 이후         | 3548            | 0.3431     | 삭제\n",
      "9    | 정도         | 3431            | 0.3766     | 삭제\n",
      "10   | 위하         | 3309            | 0.4128     | 삭제\n",
      "11   | 가지         | 3267            | 0.4256     | 삭제\n",
      "12   | 경우         | 3238            | 0.4345     | 삭제\n",
      "13   | 사실         | 3204            | 0.4450     | 삭제\n",
      "14   | 만들         | 3188            | 0.4500     | 삭제\n",
      "15   | 사람         | 3134            | 0.4671     | 삭제\n",
      "16   | 시작         | 3020            | 0.5042     | 삭제\n",
      "17   | 가능         | 2977            | 0.5185     | 삭제\n",
      "18   | 대하         | 2917            | 0.5389     | 삭제\n",
      "19   | 자신         | 2884            | 0.5503     | 삭제\n",
      "20   | 등장         | 2780            | 0.5870     | 삭제\n",
      "21   | 모습         | 2556            | 0.6710     | 삭제\n",
      "22   | 문제         | 2516            | 0.6868     | 삭제\n",
      "23   | 부분         | 2340            | 0.7593     | 삭제\n",
      "24   | 이것         | 2264            | 0.7923     | 삭제\n",
      "25   | 그것         | 1876            | 0.9803     | 삭제\n",
      "26   | 자연         | 823             | 1.8042     | 애매\n",
      "27   | 대학교        | 295             | 2.8302     | 애매\n",
      "28   | 저것         | 136             | 3.6045     | 애매\n",
      "29   | 인공지능       | 7               | 6.5713     | 키워드\n",
      "30   | 하다         | 2               | 7.8240     | 애매\n",
      "31   | 되다         | 2               | 7.8240     | 애매\n",
      "32   | 않다         | 1               | 8.5172     | 애매\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
