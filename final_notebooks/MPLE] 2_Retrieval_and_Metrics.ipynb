{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T11:07:27.101231Z",
     "start_time": "2025-12-09T11:07:26.079949Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install statsmodels",
   "id": "de508a42094311c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (0.14.6)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from statsmodels) (2.3.5)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from statsmodels) (1.16.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cse\\desktop\\xeoxaxeo\\nlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T11:19:47.594324Z",
     "start_time": "2025-12-09T11:16:34.597854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = os.path.join('..')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'final_data')\n",
    "DB_DIR = os.path.join(BASE_DIR, 'final_database')\n",
    "SAMPLE_PATH = os.path.join(DATA_DIR, 'sample.pkl')\n",
    "QUERIES_PATH = os.path.join(BASE_DIR, 'data', 'queries.pkl')\n",
    "QRELS_PATH = os.path.join(BASE_DIR, 'data', 'qrels.pkl')\n",
    "\n",
    "os.makedirs(DB_DIR, exist_ok=True)\n",
    "DB_PATH = os.path.join(DB_DIR, 'search_index.db')\n",
    "\n",
    "print(\"데이터 로드 중\")\n",
    "df_sample = pd.read_pickle(SAMPLE_PATH)\n",
    "\n",
    "if '_id' in df_sample.columns:\n",
    "    df_sample.set_index('_id', inplace=True)\n",
    "elif 'doc_id' in df_sample.columns:\n",
    "    df_sample.set_index('doc_id', inplace=True)\n",
    "\n",
    "df_sample.index = df_sample.index.astype(str)\n",
    "\n",
    "with open(QUERIES_PATH, 'rb') as f:\n",
    "    queries_data = pickle.load(f)\n",
    "with open(QRELS_PATH, 'rb') as f:\n",
    "    qrels_data = pickle.load(f)\n",
    "\n",
    "print(\"SQLite DB 초기화\")\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS inverted_index (\n",
    "        term TEXT,\n",
    "        doc_id TEXT,\n",
    "        tf INTEGER\n",
    "    )\n",
    "''')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_term ON inverted_index(term)')\n",
    "cursor.execute('CREATE INDEX IF NOT EXISTS idx_doc_id ON inverted_index(doc_id)')\n",
    "\n",
    "print(\"역색인 구축 (패딩 토큰 제외)\")\n",
    "data_to_insert = []\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "doc_freq = {}\n",
    "total_docs = len(df_sample)\n",
    "avg_dl = df_sample['doc_length'].mean()\n",
    "\n",
    "for doc_id, row in tqdm(df_sample.iterrows(), total=total_docs, desc=\"Indexing\"):\n",
    "    tokens = row['tokens_padded']\n",
    "\n",
    "    term_counts = {}\n",
    "    for t in tokens:\n",
    "        if set(t) == {'O'}:\n",
    "            continue\n",
    "        term_counts[t] = term_counts.get(t, 0) + 1\n",
    "\n",
    "    for term, tf in term_counts.items():\n",
    "        data_to_insert.append((term, str(doc_id), tf))\n",
    "        doc_freq[term] = doc_freq.get(term, 0) + 1\n",
    "\n",
    "    if len(data_to_insert) >= BATCH_SIZE:\n",
    "        cursor.executemany('INSERT INTO inverted_index VALUES (?, ?, ?)', data_to_insert)\n",
    "        data_to_insert = []\n",
    "\n",
    "if data_to_insert:\n",
    "    cursor.executemany('INSERT INTO inverted_index VALUES (?, ?, ?)', data_to_insert)\n",
    "\n",
    "conn.commit()\n",
    "print(\"역색인 구축 완료\")\n",
    "\n",
    "kiwi = Kiwi(num_workers=0)\n",
    "\n",
    "def tokenize_query(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        return [t.form for t in tokens if t.tag in ['NNG', 'NNP', 'VV', 'VA', 'MAG']]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "idf_cache = {}\n",
    "for term, df in doc_freq.items():\n",
    "    idf_cache[term] = math.log((total_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "\n",
    "def get_bim_score(query_tokens, doc_ids_in_sample):\n",
    "    scores = {str(doc_id): 0.0 for doc_id in doc_ids_in_sample}\n",
    "    for term in query_tokens:\n",
    "        if term not in idf_cache: continue\n",
    "        idf = idf_cache[term]\n",
    "        cursor.execute('SELECT doc_id FROM inverted_index WHERE term = ?', (term,))\n",
    "        rows = cursor.fetchall()\n",
    "        for (doc_id,) in rows:\n",
    "            if doc_id in scores:\n",
    "                scores[doc_id] += idf\n",
    "    return scores\n",
    "\n",
    "def get_bm25_score(query_tokens, doc_ids_in_sample):\n",
    "    scores = {str(doc_id): 0.0 for doc_id in doc_ids_in_sample}\n",
    "    for term in query_tokens:\n",
    "        if term not in idf_cache: continue\n",
    "        idf = idf_cache[term]\n",
    "        cursor.execute('SELECT doc_id, tf FROM inverted_index WHERE term = ?', (term,))\n",
    "        rows = cursor.fetchall()\n",
    "        for doc_id, tf in rows:\n",
    "            if doc_id in scores:\n",
    "                doc_len = df_sample.at[doc_id, 'doc_length']\n",
    "                numerator = tf * (k1 + 1)\n",
    "                denominator = tf + k1 * (1 - b + b * (doc_len / avg_dl))\n",
    "                scores[doc_id] += idf * (numerator / denominator)\n",
    "    return scores\n",
    "\n",
    "print(\"검색 실험 및 데이터셋 생성\")\n",
    "\n",
    "regression_data = []\n",
    "\n",
    "qrels_dict = {}\n",
    "for item in qrels_data:\n",
    "    qid = str(item.get('query-id'))\n",
    "    doc_id = str(item.get('corpus-id'))\n",
    "    if qid and doc_id:\n",
    "        if qid not in qrels_dict:\n",
    "            qrels_dict[qid] = []\n",
    "        qrels_dict[qid].append(doc_id)\n",
    "\n",
    "queries_map = {}\n",
    "for q in queries_data:\n",
    "    if '_id' in q:\n",
    "        queries_map[str(q['_id'])] = q['text']\n",
    "\n",
    "print(f\"매핑 확인 - Qrels: {len(qrels_dict)}, Queries: {len(queries_map)}\")\n",
    "\n",
    "match_count = 0\n",
    "skip_count_no_query = 0\n",
    "skip_count_no_doc = 0\n",
    "\n",
    "for qid, target_doc_ids in tqdm(qrels_dict.items(), desc=\"Processing\"):\n",
    "\n",
    "    if qid not in queries_map:\n",
    "        skip_count_no_query += 1\n",
    "        continue\n",
    "\n",
    "    query_text = queries_map[qid]\n",
    "    q_tokens = tokenize_query(query_text)\n",
    "\n",
    "    if not q_tokens:\n",
    "        continue\n",
    "\n",
    "    valid_targets = [d for d in target_doc_ids if d in df_sample.index]\n",
    "\n",
    "    if not valid_targets:\n",
    "        skip_count_no_doc += 1\n",
    "        continue\n",
    "\n",
    "    match_count += 1\n",
    "\n",
    "    bim_scores = get_bim_score(q_tokens, df_sample.index)\n",
    "    bim_sorted = sorted(bim_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    bim_top10 = [doc for doc, score in bim_sorted[:10]]\n",
    "\n",
    "    bm25_scores = get_bm25_score(q_tokens, df_sample.index)\n",
    "    bm25_sorted = sorted(bm25_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    bm25_top10 = [doc for doc, score in bm25_sorted[:10]]\n",
    "\n",
    "    for target_doc in valid_targets:\n",
    "        doc_len = df_sample.at[target_doc, 'doc_length']\n",
    "        q_len = len(q_tokens)\n",
    "        topic_probs = df_sample.at[target_doc, 'topic_probs']\n",
    "\n",
    "        base_row = {\n",
    "            'qid': qid,\n",
    "            'doc_id': target_doc,\n",
    "            'doc_length': doc_len,\n",
    "            'query_length': q_len,\n",
    "        }\n",
    "        for idx, prob in enumerate(topic_probs):\n",
    "            base_row[f'topic_{idx}'] = prob\n",
    "\n",
    "        row_bim = base_row.copy()\n",
    "        row_bim['model'] = 'BIM'\n",
    "        row_bim['success'] = 1 if target_doc in bim_top10 else 0\n",
    "        regression_data.append(row_bim)\n",
    "\n",
    "        row_bm25 = base_row.copy()\n",
    "        row_bm25['model'] = 'BM25'\n",
    "        row_bm25['success'] = 1 if target_doc in bm25_top10 else 0\n",
    "        regression_data.append(row_bm25)\n",
    "\n",
    "print(\"\\n[작업 결과]\")\n",
    "print(f\"매칭 성공 쿼리: {match_count}\")\n",
    "print(f\"텍스트 없는 쿼리: {skip_count_no_query}\")\n",
    "print(f\"정답 문서 없는 쿼리: {skip_count_no_doc}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "if regression_data:\n",
    "    df_regression = pd.DataFrame(regression_data)\n",
    "    save_path = os.path.join(DATA_DIR, 'regression_dataset.csv')\n",
    "    df_regression.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"저장 완료: {save_path}\")\n",
    "    print(f\"총 데이터: {len(df_regression)}건\")\n",
    "    print(\"\\n[모델별 성공률]\")\n",
    "    print(df_regression.groupby('model')['success'].mean())\n",
    "else:\n",
    "    print(\"데이터 생성 실패\")"
   ],
   "id": "94bc2f47fea52ed0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중\n",
      "SQLite DB 초기화\n",
      "역색인 구축 (패딩 토큰 제외)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|██████████| 5000/5000 [00:34<00:00, 143.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "역색인 구축 완료\n",
      "검색 실험 및 데이터셋 생성\n",
      "매핑 확인 - Qrels: 1454, Queries: 1454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1454/1454 [02:33<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[작업 결과]\n",
      "매칭 성공 쿼리: 1372\n",
      "텍스트 없는 쿼리: 0\n",
      "정답 문서 없는 쿼리: 82\n",
      "저장 완료: ..\\final_data\\regression_dataset.csv\n",
      "총 데이터: 10160건\n",
      "\n",
      "[모델별 성공률]\n",
      "model\n",
      "BIM     0.482087\n",
      "BM25    0.685236\n",
      "Name: success, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
