{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T01:10:52.748023Z",
     "start_time": "2025-12-11T01:10:49.392040Z"
    }
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from kiwipiepy import Kiwi\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = os.path.join('..')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data_final')\n",
    "CORPUS_PATH = os.path.join(BASE_DIR, 'data', 'corpus.pkl')\n",
    "QRELS_PATH = os.path.join(BASE_DIR, 'data', 'qrels.pkl')\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"전체 데이터 로드\")\n",
    "with open(CORPUS_PATH, 'rb') as f:\n",
    "    corpus_data = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(corpus_data)\n",
    "if 'text' not in df.columns and 'body' in df.columns:\n",
    "    df.rename(columns={'body': 'text'}, inplace=True)\n",
    "\n",
    "print(f\"전체 문서 수: {len(df)}개\")\n",
    "\n",
    "print(\"\\n[스마트 샘플링 시작]\")\n",
    "with open(QRELS_PATH, 'rb') as f:\n",
    "    qrels_data = pickle.load(f)\n",
    "\n",
    "id_col = '_id' if '_id' in df.columns else 'doc_id'\n",
    "if id_col not in df.columns:\n",
    "    df.reset_index(inplace=True)\n",
    "    id_col = 'index'\n",
    "\n",
    "relevant_ids = set()\n",
    "for item in qrels_data:\n",
    "    corpus_id = item.get('corpus-id')\n",
    "    if corpus_id:\n",
    "        relevant_ids.add(str(corpus_id))\n",
    "\n",
    "df[id_col] = df[id_col].astype(str)\n",
    "\n",
    "df_relevant = df[df[id_col].isin(relevant_ids)]\n",
    "df_others = df[~df[id_col].isin(relevant_ids)]\n",
    "\n",
    "print(f\"필수 포함 (정답 문서): {len(df_relevant)}개\")\n",
    "\n",
    "TARGET_N = 5000\n",
    "needed_n = TARGET_N - len(df_relevant)\n",
    "\n",
    "if needed_n > 0 and len(df_others) >= needed_n:\n",
    "    df_random = df_others.sample(n=needed_n, random_state=42)\n",
    "    df_sample = pd.concat([df_relevant, df_random])\n",
    "elif needed_n > 0:\n",
    "    df_sample = pd.concat([df_relevant, df_others])\n",
    "else:\n",
    "    df_sample = df_relevant.sample(n=TARGET_N, random_state=42)\n",
    "\n",
    "df = df_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"최종 샘플링 완료: {len(df)}개\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 로드\n",
      "전체 문서 수: 50222개\n",
      "\n",
      "[스마트 샘플링 시작]\n",
      "필수 포함 (정답 문서): 6194개\n",
      "최종 샘플링 완료: 5000개\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T01:33:45.238692Z",
     "start_time": "2025-12-11T01:10:52.770778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kiwi = Kiwi(num_workers=0)\n",
    "\n",
    "def is_repeated_special(text):\n",
    "    return bool(re.search(r'([!?.…~\\-])\\1+', text))\n",
    "\n",
    "def is_jamo(text):\n",
    "    return bool(re.search(r'[ㅋㅎㅠㅜㅡㅗㅓㅏㅣ]', text))\n",
    "\n",
    "print(\"\\n[불용어 추출: IDF 기반 분석]\")\n",
    "print(\"Step 1: 전체 5000개 문서에서 DF 계산\")\n",
    "\n",
    "target_pos = ['NNG', 'NNP', 'VV', 'VA', 'MAG']\n",
    "doc_freq = Counter()\n",
    "N = len(df)\n",
    "\n",
    "for text in tqdm(df['text'], desc=\"DF 계산\"):\n",
    "    if not isinstance(text, str):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        unique_tokens = set()\n",
    "\n",
    "        for t in tokens:\n",
    "            if is_repeated_special(t.form) or is_jamo(t.form):\n",
    "                continue\n",
    "            if t.tag in target_pos and len(t.form) > 1:\n",
    "                unique_tokens.add(t.form)\n",
    "\n",
    "        for token in unique_tokens:\n",
    "            doc_freq[token] += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"총 고유 토큰 수: {len(doc_freq):,}개\")\n",
    "\n",
    "print(\"\\nStep 2: IDF 계산 및 불용어 선정 (IDF < 1.5)\")\n",
    "idf_scores = []\n",
    "for word, df_count in doc_freq.items():\n",
    "    idf = math.log(N / df_count)\n",
    "    idf_scores.append((word, df_count, idf))\n",
    "\n",
    "idf_scores.sort(key=lambda x: x[2])\n",
    "\n",
    "stopwords = set()\n",
    "IDF_THRESHOLD = 1.5\n",
    "\n",
    "for word, df_count, idf in idf_scores:\n",
    "    if idf < IDF_THRESHOLD:\n",
    "        stopwords.add(word)\n",
    "\n",
    "print(f\"IDF < {IDF_THRESHOLD} 불용어: {len(stopwords)}개\")\n",
    "print(f\"\\n[불용어 예시 (상위 20개)]\")\n",
    "for i, (word, df_count, idf) in enumerate(idf_scores[:20], 1):\n",
    "    print(f\"{i:2}. {word:<10} | DF: {df_count:4} | IDF: {idf:.4f}\")\n",
    "\n",
    "stopwords_path = os.path.join(DATA_DIR, 'stopwords_v2.txt')\n",
    "with open(stopwords_path, 'w', encoding='utf-8') as f:\n",
    "    for word in sorted(stopwords):\n",
    "        f.write(f\"{word}\\n\")\n",
    "print(f\"\\n불용어 목록 저장: {stopwords_path}\")"
   ],
   "id": "1307b170f017424a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[불용어 추출: IDF 기반 분석]\n",
      "Step 1: 전체 5000개 문서에서 DF 계산\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DF 계산: 100%|██████████| 5000/5000 [22:51<00:00,  3.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 고유 토큰 수: 157,022개\n",
      "\n",
      "Step 2: IDF 계산 및 불용어 선정 (IDF < 1.5)\n",
      "IDF < 1.5 불용어: 346개\n",
      "\n",
      "[불용어 예시 (상위 20개)]\n",
      " 1. 나오         | DF: 3845 | IDF: 0.2627\n",
      " 2. 보이         | DF: 3680 | IDF: 0.3065\n",
      " 3. 이후         | DF: 3678 | IDF: 0.3071\n",
      " 4. 정도         | DF: 3496 | IDF: 0.3578\n",
      " 5. 위하         | DF: 3414 | IDF: 0.3816\n",
      " 6. 경우         | DF: 3318 | IDF: 0.4101\n",
      " 7. 사실         | DF: 3304 | IDF: 0.4143\n",
      " 8. 사람         | DF: 3223 | IDF: 0.4391\n",
      " 9. 만들         | DF: 3204 | IDF: 0.4450\n",
      "10. 시작         | DF: 3120 | IDF: 0.4716\n",
      "11. 따르         | DF: 3091 | IDF: 0.4809\n",
      "12. 가지         | DF: 3085 | IDF: 0.4829\n",
      "13. 대하         | DF: 3073 | IDF: 0.4868\n",
      "14. 가능         | DF: 3069 | IDF: 0.4881\n",
      "15. 모두         | DF: 3046 | IDF: 0.4956\n",
      "16. 이상         | DF: 3029 | IDF: 0.5012\n",
      "17. 자신         | DF: 3003 | IDF: 0.5098\n",
      "18. 함께         | DF: 2993 | IDF: 0.5132\n",
      "19. 가장         | DF: 2947 | IDF: 0.5287\n",
      "20. 다시         | DF: 2907 | IDF: 0.5423\n",
      "\n",
      "불용어 목록 저장: ..\\data_final\\stopwords_v2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T02:14:55.342737Z",
     "start_time": "2025-12-11T01:33:45.312960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_with_stopwords(text, stopwords):\n",
    "    if not isinstance(text, str):\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        padded = []\n",
    "        meaningful = []\n",
    "\n",
    "        for t in tokens:\n",
    "            if is_repeated_special(t.form) or is_jamo(t.form):\n",
    "                padded.append('O' * len(t.form))\n",
    "            elif t.tag in target_pos and len(t.form) > 1:\n",
    "                if t.form in stopwords:\n",
    "                    padded.append('O' * len(t.form))\n",
    "                else:\n",
    "                    padded.append(t.form)\n",
    "                    meaningful.append(t.form)\n",
    "            else:\n",
    "                padded.append('O' * len(t.form))\n",
    "\n",
    "        return padded, meaningful\n",
    "    except:\n",
    "        return [], []\n",
    "\n",
    "def extract_document_features(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "    try:\n",
    "        tokens = kiwi.tokenize(text)\n",
    "        morph_count = len(tokens)\n",
    "\n",
    "        syllable_count = len(re.sub(r'\\s+', '', text))\n",
    "\n",
    "        sentences = kiwi.split_into_sents(text)\n",
    "        sent_count = len(sentences)\n",
    "\n",
    "        avg_sent_len = syllable_count / sent_count if sent_count > 0 else 0.0\n",
    "\n",
    "        return morph_count, syllable_count, sent_count, avg_sent_len\n",
    "    except:\n",
    "        return 0, 0, 0, 0.0\n",
    "\n",
    "tqdm.pandas()\n",
    "print(\"\\n샘플 데이터 전처리 (불용어 패딩 포함)\")\n",
    "df[['tokens_padded', 'tokens_lda']] = df['text'].progress_apply(\n",
    "    lambda x: pd.Series(preprocess_with_stopwords(x, stopwords))\n",
    ")\n",
    "\n",
    "print(\"\\n문서 특성 추출\")\n",
    "df[['morph_count', 'syllable_count', 'sent_count', 'avg_sent_len']] = df['text'].progress_apply(\n",
    "    lambda x: pd.Series(extract_document_features(x))\n",
    ")\n",
    "\n",
    "df['doc_length'] = df['tokens_padded'].apply(lambda x: sum(len(t) for t in x))"
   ],
   "id": "17573161d5e5e64b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "샘플 데이터 전처리 (불용어 패딩 포함)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [14:38<00:00,  5.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 특성 추출\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [26:30<00:00,  3.14it/s]  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T02:15:28.447156Z",
     "start_time": "2025-12-11T02:14:55.470763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nLDA 토픽 모델 학습\")\n",
    "lda_tokens = df['tokens_lda'].tolist()\n",
    "dictionary = corpora.Dictionary(lda_tokens)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in lda_tokens]\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    "    random_state=42,\n",
    "    passes=5,\n",
    "    alpha='auto'\n",
    ")\n",
    "\n",
    "def get_topic_probs(tokens):\n",
    "    if not tokens:\n",
    "        return [0.1] * 10\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topics = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    topic_vec = [0.0] * 10\n",
    "    for topic_id, prob in topics:\n",
    "        topic_vec[topic_id] = prob\n",
    "    return topic_vec\n",
    "\n",
    "print(\"토픽 확률 추론\")\n",
    "df['topic_probs'] = df['tokens_lda'].progress_apply(get_topic_probs)\n",
    "\n",
    "df['dominant_topic'] = df['topic_probs'].apply(lambda x: np.argmax(x))\n",
    "df['dominant_prob'] = df['topic_probs'].apply(lambda x: np.max(x))\n",
    "\n",
    "save_path = os.path.join(DATA_DIR, 'sampled_data_v2.pkl')\n",
    "model_path = os.path.join(DATA_DIR, 'lda_model_v2.model')\n",
    "\n",
    "df.to_pickle(save_path)\n",
    "lda_model.save(model_path)"
   ],
   "id": "b9952f7d1b60867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA 토픽 모델 학습\n",
      "토픽 확률 추론\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:03<00:00, 1360.59it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T02:15:28.464588Z",
     "start_time": "2025-12-11T02:15:28.452467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n작업 완료\")\n",
    "print(f\"저장 경로: {save_path}\")\n",
    "print(f\"최종 문서 수: {len(df)}개\")\n",
    "print(f\"불용어 개수: {len(stopwords)}개\")\n",
    "\n",
    "print(f\"\\n[문서 길이 통계 (doc_length)]\")\n",
    "print(f\"  - 평균: {df['doc_length'].mean():.1f}\")\n",
    "print(f\"  - 최소: {df['doc_length'].min()}\")\n",
    "print(f\"  - 중앙값: {df['doc_length'].median():.0f}\")\n",
    "print(f\"  - 최대: {df['doc_length'].max()}\")\n",
    "\n",
    "print(f\"\\n[형태소 개수 (morph_count)]\")\n",
    "print(f\"  - 평균: {df['morph_count'].mean():.1f}\")\n",
    "print(f\"  - 중앙값: {df['morph_count'].median():.0f}\")\n",
    "\n",
    "print(f\"\\n[음절 수 (syllable_count)]\")\n",
    "print(f\"  - 평균: {df['syllable_count'].mean():.1f}\")\n",
    "print(f\"  - 중앙값: {df['syllable_count'].median():.0f}\")\n",
    "\n",
    "print(f\"\\n[문장 개수 (sent_count)]\")\n",
    "print(f\"  - 평균: {df['sent_count'].mean():.1f}\")\n",
    "print(f\"  - 중앙값: {df['sent_count'].median():.0f}\")\n",
    "\n",
    "print(f\"\\n[평균 문장 길이 (avg_sent_len)]\")\n",
    "print(f\"  - 평균: {df['avg_sent_len'].mean():.1f}\")\n",
    "print(f\"  - 중앙값: {df['avg_sent_len'].median():.0f}\")\n",
    "\n",
    "print(\"\\n[LDA 토픽 예시]\")\n",
    "for idx, topic in lda_model.print_topics(3):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "print(\"\\n[토픽 분포]\")\n",
    "topic_dist = df['dominant_topic'].value_counts().sort_index()\n",
    "for topic_id, count in topic_dist.items():\n",
    "    print(f\"Topic {topic_id}: {count}개 ({count/len(df)*100:.1f}%)\")"
   ],
   "id": "cf97b38003247e23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "작업 완료\n",
      "저장 경로: ..\\data_final\\sampled_data_v2.pkl\n",
      "최종 문서 수: 5000개\n",
      "불용어 개수: 346개\n",
      "\n",
      "[문서 길이 통계 (doc_length)]\n",
      "  - 평균: 7873.0\n",
      "  - 최소: 0\n",
      "  - 중앙값: 4366\n",
      "  - 최대: 102419\n",
      "\n",
      "[형태소 개수 (morph_count)]\n",
      "  - 평균: 5053.2\n",
      "  - 중앙값: 2777\n",
      "\n",
      "[음절 수 (syllable_count)]\n",
      "  - 평균: 7514.7\n",
      "  - 중앙값: 4174\n",
      "\n",
      "[문장 개수 (sent_count)]\n",
      "  - 평균: 154.0\n",
      "  - 중앙값: 86\n",
      "\n",
      "[평균 문장 길이 (avg_sent_len)]\n",
      "  - 평균: 55.7\n",
      "  - 중앙값: 47\n",
      "\n",
      "[LDA 토픽 예시]\n",
      "Topic 2: 0.024*\"시즌\" + 0.021*\"경기\" + 0.015*\"선수\" + 0.011*\"우승\" + 0.009*\"투수\" + 0.008*\"리그\" + 0.008*\"감독\" + 0.007*\"야구\" + 0.007*\"삼성\" + 0.006*\"홈런\"\n",
      "Topic 0: 0.007*\"영화\" + 0.005*\"포켓몬\" + 0.004*\"코너\" + 0.004*\"소닉\" + 0.003*\"출연\" + 0.003*\"디자인\" + 0.003*\"애니메이션\" + 0.003*\"방영\" + 0.003*\"세대\" + 0.003*\"요리\"\n",
      "Topic 5: 0.010*\"스킬\" + 0.009*\"레벨\" + 0.008*\"무기\" + 0.006*\"증가\" + 0.006*\"데미지\" + 0.005*\"카드\" + 0.005*\"마법\" + 0.005*\"유저\" + 0.005*\"보스\" + 0.005*\"공격력\"\n",
      "\n",
      "[토픽 분포]\n",
      "Topic 0: 679개 (13.6%)\n",
      "Topic 1: 822개 (16.4%)\n",
      "Topic 2: 295개 (5.9%)\n",
      "Topic 3: 391개 (7.8%)\n",
      "Topic 4: 445개 (8.9%)\n",
      "Topic 5: 847개 (16.9%)\n",
      "Topic 6: 503개 (10.1%)\n",
      "Topic 7: 204개 (4.1%)\n",
      "Topic 8: 538개 (10.8%)\n",
      "Topic 9: 276개 (5.5%)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:36:23.271435Z",
     "start_time": "2025-12-11T15:36:23.243667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n[LDA 토픽 목록]\")\n",
    "for idx, topic in lda_model.print_topics(9):\n",
    "    print(f\"Topic {idx}: {topic}\")"
   ],
   "id": "7d6835a946ba3861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LDA 토픽 목록]\n",
      "Topic 2: 0.024*\"시즌\" + 0.021*\"경기\" + 0.015*\"선수\" + 0.011*\"우승\" + 0.009*\"투수\" + 0.008*\"리그\" + 0.008*\"감독\" + 0.007*\"야구\" + 0.007*\"삼성\" + 0.006*\"홈런\"\n",
      "Topic 7: 0.029*\"경기\" + 0.019*\"선수\" + 0.016*\"리그\" + 0.016*\"시즌\" + 0.011*\"우승\" + 0.008*\"월드컵\" + 0.007*\"축구\" + 0.007*\"대회\" + 0.007*\"감독\" + 0.007*\"진출\"\n",
      "Topic 9: 0.014*\"버스\" + 0.008*\"서울\" + 0.008*\"노선\" + 0.007*\"도시\" + 0.007*\"대학\" + 0.006*\"학교\" + 0.005*\"후보\" + 0.005*\"차량\" + 0.004*\"운행\" + 0.004*\"학생\"\n",
      "Topic 8: 0.010*\"방송\" + 0.006*\"멤버\" + 0.004*\"대통령\" + 0.004*\"뉴스\" + 0.004*\"정부\" + 0.004*\"앨범\" + 0.004*\"세대\" + 0.004*\"그룹\" + 0.003*\"철학\" + 0.003*\"프로그램\"\n",
      "Topic 3: 0.003*\"경찰\" + 0.003*\"대통령\" + 0.003*\"정치\" + 0.003*\"사회\" + 0.003*\"갤러리\" + 0.002*\"여성\" + 0.002*\"정부\" + 0.002*\"판사\" + 0.002*\"조사\" + 0.002*\"법원\"\n",
      "Topic 4: 0.003*\"중국\" + 0.003*\"대만\" + 0.003*\"영웅\" + 0.003*\"전쟁\" + 0.002*\"파괴\" + 0.002*\"신화\" + 0.002*\"신라\" + 0.002*\"고구려\" + 0.002*\"백제\" + 0.002*\"마법\"\n",
      "Topic 1: 0.004*\"아버지\" + 0.004*\"사랑\" + 0.003*\"친구\" + 0.002*\"어머니\" + 0.002*\"기억\" + 0.002*\"작중\" + 0.002*\"대사\" + 0.002*\"여자\" + 0.002*\"좋아하\" + 0.002*\"남자\"\n",
      "Topic 0: 0.007*\"영화\" + 0.005*\"포켓몬\" + 0.004*\"코너\" + 0.004*\"소닉\" + 0.003*\"출연\" + 0.003*\"디자인\" + 0.003*\"애니메이션\" + 0.003*\"방영\" + 0.003*\"세대\" + 0.003*\"요리\"\n",
      "Topic 5: 0.010*\"스킬\" + 0.009*\"레벨\" + 0.008*\"무기\" + 0.006*\"증가\" + 0.006*\"데미지\" + 0.005*\"카드\" + 0.005*\"마법\" + 0.005*\"유저\" + 0.005*\"보스\" + 0.005*\"공격력\"\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
